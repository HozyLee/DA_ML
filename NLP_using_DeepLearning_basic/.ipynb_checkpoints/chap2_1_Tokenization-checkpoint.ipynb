{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥 러닝을 이용한 자연어 처리 입문\n",
    "\n",
    "https://wikidocs.net/22660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 텍스트 전처리(text preprocessing)\n",
    "\n",
    "분석에 앞서 텍스트를 사전에 분류하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-1 토큰화(tokenization)\n",
    "\n",
    "\n",
    "    토큰 : corpus 에서 의미가 있는 단어로 주로 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 단어 토큰화(word tokenization)\n",
    "\n",
    "토큰의 기준을 단어로 하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 토큰화 중 생기는 선택의 순간\n",
    "\n",
    "토큰화 과정에서 생기는 애매한 기준의 경우\n",
    "\n",
    "해당 데이터의 용도(목적)에 따라 다르게 토큰화 해야한다.\n",
    "\n",
    "ex) 아스트로피(') 의 경우 : Don't, Don t, Do n't 등등.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenizer 함수는 Do n't 로 분류\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer 함수는 Don ' t 로 처리\n",
    "## 구두점을 별도로 분류하는 특징을 갖기 때문\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 토큰화에서 고려해야할 사항\n",
    "\n",
    "    1. 구두점이나 특수문자를 단순 제외해서는 안된다.\n",
    "            - ex) 아포스트로피 I'm : I + am(접어)\n",
    "    2. 줄임말과 단어 내에 띄어쓰기가 있는 경우.\n",
    "            - ex) New York, rock 'n' roll.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표준 Tokenization 예제\n",
    "\n",
    "가장 표준적인 Penn Treebank Tokenization 규칙\n",
    "\n",
    "    1. 하이푼으로 구성된 단어는 하나로 유지\n",
    "                - ex) home-based\n",
    "    2. dosen't 와 같이 아포스트로피로 접어가 존재하는 단어는 분리\n",
    "                - ex) dosen't => do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer=  TreebankWordTokenizer()\n",
    "text= \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 문장 토큰화\n",
    "\n",
    "    토큰의 단위가 문장인 경우\n",
    "    \n",
    "    문장을 잘라낼 수 있는 기준\n",
    "    ! 나 ? \n",
    "    . : 문장의 끝이 아니더라도 존재 가능\n",
    "    \n",
    "       ex1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 dailybugle@naver.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.\n",
    "\n",
    "        ex2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "# 중간에 온점이 여러번 등장하는 경우\n",
    "# nltk 는 문장 토큰화 시 단순히 온점(.)으로 구분하지 않기 때문에 잘 분류하였다.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 이진 분류기\n",
    "\n",
    "문장토큰화에서 예외사항을 발생시키는 온점의 처리를 위함\n",
    "두 개의 클래스로 분류하는 이진 분류기 사용\n",
    "\n",
    "    1.약어(Abbreivation) : 온점이 단어의 일부분일 경우\n",
    "    2.구분자(boundary) : 문장을 구분하는 경우\n",
    "    \n",
    "1 번 케이스를 잘 구분해주어야 한다.\n",
    "\n",
    "따라서, 이진분류기 구현에서는 약어사전(Abbreviation Dictionary)사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 한국어에서의 토큰화의 어려움.\n",
    "\n",
    "영어권 : 합성어(New York)와 줄임말(he's)만 잘 처리한다면 space로 단어 토큰화가 대부분 잘 작동한다.\n",
    "\n",
    "한국어 : 어절(띄어쓰기 단위) 존재로 space 만으로 토큰화 무리\n",
    "\n",
    "\n",
    "    1. 조사가 띄어쓰기 없이 바로 붙는다.\n",
    "            ex) 그가, 그를, 그는, 그와.. : 같은 '그'를 나타낸다.\n",
    "    2. 한국어는 띄어쓰기가 잘 지켜지지 않는다.\n",
    "            영어권 : 풀어쓰기 방식(띄어쓰기 필수)\n",
    "            한국어 : 모아쓰기 방식(띄어쓰기 없어도 어느 정도 이해 가능)\n",
    "            \n",
    "                eX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
    "\n",
    "                eX2) Tobeornottobethatisthequestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 품사 부착(Part-of-speech tagging)\n",
    "\n",
    "단어 표기가 같지만 의미(품사에 따라)가 달라지는 경우\n",
    "\n",
    "단어의 정확한 의미를 위해서는 품사 부착이 필요하다.\n",
    "\n",
    "ex) fly(동사) : 날다 / fly(명사) : 파리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. NLTK 와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
    "\n",
    "nltk\n",
    "    - 품사 부착 기능 : Penn Treebank POS Tags 기준 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================word tokenizing result====================\n",
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "====================taging result====================\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 1. 토큰화 수행(여기서는 단어 토큰화)\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text= \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "\n",
    "x = word_tokenize(text)\n",
    "print('=' * 20 + 'word tokenizing result' + '=' * 20)\n",
    "print(x)\n",
    "\n",
    "# 2. 품사 태깅\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "print('=' * 20 + 'taging result' + '=' * 20)\n",
    "print(pos_tag(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 형태소 분석기\n",
    "\n",
    "정확히는 단어(word) 토큰화가 아니라 형태소(morpheme) 단위로 토큰화 수행\n",
    "\n",
    "KoNLPY 패키지\n",
    "\n",
    "각 분석기는 방법이 다르기 때문에 용도에 맞는 분석기를 사용해야한다.\n",
    "\n",
    "    Okt\n",
    "    메캅\n",
    "    코모란\n",
    "    한나눔\n",
    "    꼬꼬마"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기능\n",
    "\n",
    "분석기 객체\n",
    "    \n",
    "    .morphs : 형태소 추출\n",
    "    .pos : 품사 부착(part-of-speech tagging)\n",
    "    .nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요', '.']\n",
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요', '.']\n"
     ]
    }
   ],
   "source": [
    "# morphs 를 이용한 형태소 토큰화\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"))\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb'), ('.', 'Punctuation')]\n",
      "\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# pos 를 이용한 형태소 토큰화  + 품사 태깅\n",
    "\n",
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"))\n",
    "print()\n",
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n",
      "\n",
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# nouns 를 사용한 명사 추출\n",
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"))\n",
    "print()\n",
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
